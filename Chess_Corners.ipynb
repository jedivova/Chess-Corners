{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widjets\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from time import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import json\n",
    "\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.7.0, catalyst: 20.08.2\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "import catalyst\n",
    "from catalyst import utils\n",
    "\n",
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_train_data():\n",
    "    #X_test = np.load('idchess_task/xtest.npy')\n",
    "    X_train = np.load(r'Z:\\WORK\\test tasks\\idchess\\idchess_task/xtrain.npy').astype(np.uint8)\n",
    "    Y_train = np.load(r'Z:\\WORK\\test tasks\\idchess\\idchess_task/ytrain.npy')\n",
    "\n",
    "    #preprocess Y_train\n",
    "    Y_train = np.around(np.clip(Y_train*255, 0, 255)).astype(int).reshape(-1,4,2)\n",
    "\n",
    "    print(Y_train.shape, X_train.shape)\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "def Get_Gauss_point():\n",
    "    \"\"\"\n",
    "    Get Gauss blurred point on 25x25 grid\n",
    "    \"\"\"\n",
    "    x, y = np.mgrid[0:1.0:25j, 0:1.0:25j]\n",
    "    xy = np.column_stack([x.flat, y.flat])\n",
    "    mask = np.zeros(25 * 25)\n",
    "    COV=np.diag([0.03,0.03])\n",
    "    Mean=[0.5,0.5]\n",
    "    Gauss = multivariate_normal.pdf(xy, mean=Mean, cov=COV)\n",
    "    Gauss *= np.sqrt(np.linalg.det(COV)*(2*np.pi)**2)  #normalize to 1(formulae Gauss)\n",
    "    Gauss = Gauss.reshape((25, 25))\n",
    "    return Gauss\n",
    "\n",
    "Gauss = Get_Gauss_point()\n",
    "\n",
    "\n",
    "def Get_mask(points):\n",
    "    \"\"\"\n",
    "    Get mask for image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : ndarray of shape [n_samples, 1, 2]\n",
    "        array with point coordinates shaped [x,y]\n",
    "    Returns\n",
    "        -------\n",
    "        mask:\n",
    "            256x256 mask with gauss blurred dots\n",
    "        \"\"\"\n",
    "    points = points.astype(int)\n",
    "    global Gauss\n",
    "    mask = np.zeros((256+12*2,256+12*2)) #центр гауссианы смещен влево вверх по 12 пикселей, \n",
    "                                    #поэтому можно расширить маску по 12 с каждой стороны и потом обрезать\n",
    "    for point in points:\n",
    "        x,y = point\n",
    "        mask[y:y+25,x:x+25] += Gauss\n",
    "        \n",
    "    mask = mask[12:-12,12:-12]\n",
    "    np.clip(mask, 0, 1)\n",
    "    return mask\n",
    "\n",
    "def Get_coords(mask):\n",
    "    \"\"\"\n",
    "    get coordinates of angles of chess board\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : ndarray of shape [height, width]\n",
    "        predicted mask \n",
    "    Returns\n",
    "        -------\n",
    "        coords:\n",
    "            ndarray of shape [n_dots, 1, 2]\n",
    "    \"\"\"\n",
    "    ret, labels = cv2.connectedComponents((mask>0.9).astype(np.uint8))\n",
    "    predicted_points = []\n",
    "    for i in range(1,ret+1):\n",
    "        dots = np.argwhere(labels==i)\n",
    "        N = dots.shape[0]\n",
    "        if N>=4: #если  \"кучка точек состоит из более чем 4 пикселей\"\n",
    "            coord = (dots.sum(axis=0)/N).astype(int)\n",
    "            predicted_points.append(coord)\n",
    "    \n",
    "    #assert len(predicted_points)==4, f'num of predicted dots is {len(predicted_points)}'\n",
    "    return np.vstack(predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images,\n",
    "        coords = None,\n",
    "        transforms=None\n",
    "    ) -> None:\n",
    "        self.images = images\n",
    "        self.coords = coords\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        img = self.images[idx]\n",
    "        image = np.concatenate((img,img,img),axis=2)\n",
    "        result = {\"image\": image}\n",
    "        \n",
    "        if self.coords is not None:\n",
    "            mask = Get_mask(self.coords[idx])\n",
    "            result[\"mask\"] = mask\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            result = self.transforms(**result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, RandomRotate90, Normalize, Compose, HueSaturationValue,\n",
    "    ShiftScaleRotate, RandomGamma, IAAAdditiveGaussianNoise, GaussNoise,\n",
    "    JpegCompression, Flip, Transpose, MedianBlur,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast,\n",
    "    MotionBlur, OneOf, Blur, CLAHE, CropNonEmptyMaskIfExists,\n",
    "    ElasticTransform, GridDistortion, OpticalDistortion, IAAAffine)\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "\n",
    "def post_transform():\n",
    "    return Compose([\n",
    "        Normalize(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "def train_transforms(p = 0.3):\n",
    "    transforms = [\n",
    "        RandomRotate90(p=p),\n",
    "        Flip(p=p),\n",
    "        Transpose(p=p),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1),\n",
    "            Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        CLAHE(p=p),\n",
    "        IAASharpen(p=p),\n",
    "        IAAEmboss(p=p),\n",
    "        RandomBrightnessContrast(\n",
    "            brightness_limit=0.5,\n",
    "            contrast_limit=0.5,\n",
    "            p=p\n",
    "        ),\n",
    "        RandomGamma(gamma_limit=(85, 115), p=p),\n",
    "        JpegCompression(quality_lower=75, p=p),\n",
    "        post_transform()\n",
    "    ]\n",
    "    transforms = Compose(transforms)\n",
    "    return transforms\n",
    "\n",
    "\n",
    "def valid_transforms():\n",
    "    return post_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15137, 4, 2) (15137, 256, 256, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2b0a241b0c4ab5aef10a19f9a5799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=15136), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# На посмотреть\n",
    "X_train, Y_train = Load_train_data()\n",
    "train_tr = train_transforms()\n",
    "\n",
    "@interact(i=(0,len(X_train)-1,1))\n",
    "def tst(i=0):\n",
    "    points = Y_train[i].copy()\n",
    "    img = X_train[i]\n",
    "    image = np.concatenate((img,img,img),axis=2)\n",
    "    result = {'image': image, 'mask':Get_mask(Y_train[i].copy())}\n",
    "    output = train_tr(**result)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(output['image'][0])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(output['mask'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    random_state: int,\n",
    "    valid_size: float = 0.2,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4,\n",
    "    train_transforms_fn = None,\n",
    "    valid_transforms_fn = None,\n",
    ") -> dict:\n",
    "\n",
    "    X, Y = Load_train_data()\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "    \n",
    "\n",
    "    # Creates our train dataset\n",
    "    train_dataset = SegmentationDataset(\n",
    "      images = X_train,#.tolist(),\n",
    "      coords = Y_train,\n",
    "      transforms = train_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Creates our valid dataset\n",
    "    valid_dataset = SegmentationDataset(\n",
    "      images = X_val,\n",
    "      coords = Y_val,\n",
    "      transforms = valid_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Catalyst uses normal torch.data.DataLoader\n",
    "    train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "      valid_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      drop_last=True,\n",
    "    )\n",
    "\n",
    "    # And excpect to get an OrderedDict of loaders\n",
    "    loaders = collections.OrderedDict()\n",
    "    loaders[\"train\"] = train_loader\n",
    "    loaders[\"valid\"] = valid_loader\n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15137, 4, 2) (15137, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size=24\n",
    "loaders = get_loaders(\n",
    "    random_state=SEED,\n",
    "    train_transforms_fn=train_transforms(),\n",
    "    valid_transforms_fn=post_transform(),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# We will use Feature Pyramid Network with pre-trained ResNeXt50 backbone\n",
    "model = smp.FPN(encoder_name=\"efficientnet-b1\", classes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import DiceLoss, IoULoss\n",
    "\n",
    "# we have multiple criterions\n",
    "criterion = {\n",
    "    \"dice\": DiceLoss(),\n",
    "    \"iou\": IoULoss(),\n",
    "    \"bce\": nn.BCEWithLogitsLoss()\n",
    "}\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from catalyst.contrib.nn import RAdam, Lookahead\n",
    "\n",
    "learning_rate = 0.001\n",
    "encoder_learning_rate = 0.0005\n",
    "\n",
    "# Since we use a pre-trained encoder, we will reduce the learning rate on it.\n",
    "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n",
    "\n",
    "# This function removes weight_decay for biases and applies our layerwise_params\n",
    "model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n",
    "\n",
    "# Catalyst has new SOTA optimizers out of box\n",
    "base_optimizer = RAdam(model_params, lr=learning_rate, weight_decay=0.0003)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "num_epochs = 10\n",
    "logdir = \"./logs/segmentation\"\n",
    "\n",
    "device = utils.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "\n",
    "# by default SupervisedRunner uses \"features\" and \"targets\",\n",
    "# in our case we get \"image\" and \"mask\" keys in dataset __getitem__\n",
    "runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6724), started 0:00:39 ago. (Use '!kill 6724' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train):   0% 1/536 [00:02<25:47,  2.89s/it, dice=0.014, iou=0.007, loss=3.083, loss_bce=1.379, loss_dice=0.986, loss_iou=0.993]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning:\n",
      "\n",
      "This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train): 100% 536/536 [05:46<00:00,  1.55it/s, dice=0.542, iou=0.372, loss=1.110, loss_bce=0.029, loss_dice=0.458, loss_iou=0.628]\n",
      "1/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.19it/s, dice=0.582, iou=0.410, loss=1.029, loss_bce=0.026, loss_dice=0.418, loss_iou=0.590]\n",
      "[2020-11-21 20:51:20,727] \n",
      "1/10 * Epoch 1 (_base): lr=0.0005 | momentum=0.9000\n",
      "1/10 * Epoch 1 (train): dice=0.3991 | iou=0.2633 | loss=1.3787 | loss_bce=0.0514 | loss_dice=0.6009 | loss_iou=0.7367\n",
      "1/10 * Epoch 1 (valid): dice=0.5910 | iou=0.4194 | loss=1.0100 | loss_bce=0.0254 | loss_dice=0.4090 | loss_iou=0.5806\n",
      "2/10 * Epoch (train): 100% 536/536 [05:40<00:00,  1.57it/s, dice=0.602, iou=0.430, loss=0.991, loss_bce=0.029, loss_dice=0.398, loss_iou=0.570]\n",
      "2/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.18it/s, dice=0.614, iou=0.443, loss=0.963, loss_bce=0.026, loss_dice=0.386, loss_iou=0.557]\n",
      "[2020-11-21 20:57:19,771] \n",
      "2/10 * Epoch 2 (_base): lr=0.0005 | momentum=0.9000\n",
      "2/10 * Epoch 2 (train): dice=0.5763 | iou=0.4049 | loss=1.0413 | loss_bce=0.0282 | loss_dice=0.4237 | loss_iou=0.5951\n",
      "2/10 * Epoch 2 (valid): dice=0.6121 | iou=0.4411 | loss=0.9686 | loss_bce=0.0272 | loss_dice=0.3879 | loss_iou=0.5589\n",
      "3/10 * Epoch (train): 100% 536/536 [05:41<00:00,  1.57it/s, dice=0.609, iou=0.438, loss=0.976, loss_bce=0.029, loss_dice=0.391, loss_iou=0.562]\n",
      "3/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.18it/s, dice=0.622, iou=0.452, loss=0.949, loss_bce=0.029, loss_dice=0.378, loss_iou=0.548]\n",
      "[2020-11-21 21:03:19,696] \n",
      "3/10 * Epoch 3 (_base): lr=0.0005 | momentum=0.9000\n",
      "3/10 * Epoch 3 (train): dice=0.5972 | iou=0.4258 | loss=1.0000 | loss_bce=0.0287 | loss_dice=0.4028 | loss_iou=0.5742\n",
      "3/10 * Epoch 3 (valid): dice=0.6187 | iou=0.4480 | loss=0.9576 | loss_bce=0.0303 | loss_dice=0.3813 | loss_iou=0.5520\n",
      "4/10 * Epoch (train): 100% 536/536 [05:41<00:00,  1.57it/s, dice=0.605, iou=0.433, loss=0.986, loss_bce=0.031, loss_dice=0.395, loss_iou=0.567]\n",
      "4/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.14it/s, dice=0.627, iou=0.457, loss=0.940, loss_bce=0.029, loss_dice=0.373, loss_iou=0.543]\n",
      "[2020-11-21 21:09:20,297] \n",
      "4/10 * Epoch 4 (_base): lr=0.0001 | momentum=0.9000\n",
      "4/10 * Epoch 4 (train): dice=0.6062 | iou=0.4350 | loss=0.9822 | loss_bce=0.0293 | loss_dice=0.3938 | loss_iou=0.5650\n",
      "4/10 * Epoch 4 (valid): dice=0.6230 | iou=0.4525 | loss=0.9484 | loss_bce=0.0300 | loss_dice=0.3770 | loss_iou=0.5475\n",
      "5/10 * Epoch (train): 100% 536/536 [05:40<00:00,  1.57it/s, dice=0.611, iou=0.440, loss=0.974, loss_bce=0.030, loss_dice=0.389, loss_iou=0.560]\n",
      "5/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.21it/s, dice=0.629, iou=0.459, loss=0.934, loss_bce=0.028, loss_dice=0.371, loss_iou=0.541]\n",
      "[2020-11-21 21:15:19,848] \n",
      "5/10 * Epoch 5 (_base): lr=0.0001 | momentum=0.9000\n",
      "5/10 * Epoch 5 (train): dice=0.6118 | iou=0.4407 | loss=0.9708 | loss_bce=0.0291 | loss_dice=0.3882 | loss_iou=0.5593\n",
      "5/10 * Epoch 5 (valid): dice=0.6255 | iou=0.4551 | loss=0.9423 | loss_bce=0.0288 | loss_dice=0.3745 | loss_iou=0.5449\n",
      "6/10 * Epoch (train): 100% 536/536 [05:40<00:00,  1.57it/s, dice=0.619, iou=0.448, loss=0.956, loss_bce=0.029, loss_dice=0.381, loss_iou=0.552]\n",
      "6/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.20it/s, dice=0.630, iou=0.460, loss=0.933, loss_bce=0.029, loss_dice=0.370, loss_iou=0.540]\n",
      "[2020-11-21 21:21:19,086] \n",
      "6/10 * Epoch 6 (_base): lr=0.0001 | momentum=0.9000\n",
      "6/10 * Epoch 6 (train): dice=0.6134 | iou=0.4424 | loss=0.9676 | loss_bce=0.0292 | loss_dice=0.3866 | loss_iou=0.5576\n",
      "6/10 * Epoch 6 (valid): dice=0.6265 | iou=0.4562 | loss=0.9408 | loss_bce=0.0294 | loss_dice=0.3735 | loss_iou=0.5438\n",
      "7/10 * Epoch (train): 100% 536/536 [05:50<00:00,  1.53it/s, dice=0.611, iou=0.440, loss=0.973, loss_bce=0.030, loss_dice=0.389, loss_iou=0.560]\n",
      "7/10 * Epoch (valid): 100% 94/94 [00:19<00:00,  4.93it/s, dice=0.630, iou=0.460, loss=0.933, loss_bce=0.029, loss_dice=0.370, loss_iou=0.540]\n",
      "[2020-11-21 21:27:29,460] \n",
      "7/10 * Epoch 7 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "7/10 * Epoch 7 (train): dice=0.6146 | iou=0.4436 | loss=0.9651 | loss_bce=0.0292 | loss_dice=0.3854 | loss_iou=0.5564\n",
      "7/10 * Epoch 7 (valid): dice=0.6268 | iou=0.4565 | loss=0.9407 | loss_bce=0.0300 | loss_dice=0.3732 | loss_iou=0.5435\n",
      "8/10 * Epoch (train): 100% 536/536 [05:55<00:00,  1.51it/s, dice=0.619, iou=0.448, loss=0.957, loss_bce=0.029, loss_dice=0.381, loss_iou=0.552]\n",
      "8/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  5.04it/s, dice=0.631, iou=0.461, loss=0.931, loss_bce=0.029, loss_dice=0.369, loss_iou=0.539]\n",
      "[2020-11-21 21:33:44,615] \n",
      "8/10 * Epoch 8 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "8/10 * Epoch 8 (train): dice=0.6161 | iou=0.4452 | loss=0.9620 | loss_bce=0.0291 | loss_dice=0.3839 | loss_iou=0.5548\n",
      "8/10 * Epoch 8 (valid): dice=0.6277 | iou=0.4574 | loss=0.9385 | loss_bce=0.0294 | loss_dice=0.3723 | loss_iou=0.5426\n",
      "9/10 * Epoch (train): 100% 536/536 [05:58<00:00,  1.50it/s, dice=0.620, iou=0.450, loss=0.952, loss_bce=0.028, loss_dice=0.380, loss_iou=0.550]\n",
      "9/10 * Epoch (valid): 100% 94/94 [00:19<00:00,  4.91it/s, dice=0.632, iou=0.462, loss=0.930, loss_bce=0.028, loss_dice=0.368, loss_iou=0.538]\n",
      "[2020-11-21 21:40:02,418] \n",
      "9/10 * Epoch 9 (_base): lr=3.125e-05 | momentum=0.9000\n",
      "9/10 * Epoch 9 (train): dice=0.6163 | iou=0.4454 | loss=0.9616 | loss_bce=0.0292 | loss_dice=0.3837 | loss_iou=0.5546\n",
      "9/10 * Epoch 9 (valid): dice=0.6278 | iou=0.4575 | loss=0.9380 | loss_bce=0.0292 | loss_dice=0.3722 | loss_iou=0.5425\n",
      "10/10 * Epoch (train): 100% 536/536 [05:49<00:00,  1.53it/s, dice=0.616, iou=0.445, loss=0.963, loss_bce=0.029, loss_dice=0.384, loss_iou=0.555]\n",
      "10/10 * Epoch (valid): 100% 94/94 [00:18<00:00,  4.95it/s, dice=0.632, iou=0.462, loss=0.930, loss_bce=0.029, loss_dice=0.368, loss_iou=0.538]\n",
      "[2020-11-21 21:46:11,671] \n",
      "10/10 * Epoch 10 (_base): lr=7.813e-06 | momentum=0.9000\n",
      "10/10 * Epoch 10 (train): dice=0.6168 | iou=0.4460 | loss=0.9606 | loss_bce=0.0292 | loss_dice=0.3832 | loss_iou=0.5540\n",
      "10/10 * Epoch 10 (valid): dice=0.6279 | iou=0.4576 | loss=0.9380 | loss_bce=0.0293 | loss_dice=0.3721 | loss_iou=0.5424\n",
      "Top best models:\n",
      "logs\\segmentation\\checkpoints/train.10.pth\t0.6279\n"
     ]
    }
   ],
   "source": [
    "from catalyst.dl import DiceCallback, IouCallback, \\\n",
    "  CriterionCallback, MetricAggregationCallback\n",
    "\n",
    "callbacks = [\n",
    "    # Each criterion is calculated separately.\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_dice\",\n",
    "        criterion_key=\"dice\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_iou\",\n",
    "        criterion_key=\"iou\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_bce\",\n",
    "        criterion_key=\"bce\"\n",
    "    ),\n",
    "\n",
    "    # And only then we aggregate everything into one loss.\n",
    "    MetricAggregationCallback(\n",
    "        prefix=\"loss\",\n",
    "        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n",
    "        # because we want weighted sum, we need to add scale for each loss\n",
    "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
    "    ),\n",
    "\n",
    "    # metrics\n",
    "    DiceCallback(input_key=\"mask\"),\n",
    "    IouCallback(input_key=\"mask\"),\n",
    "]\n",
    "\n",
    "model.train()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    # our dataloaders\n",
    "    loaders=loaders,\n",
    "    # We can specify the callbacks list for the experiment;\n",
    "    callbacks=callbacks,\n",
    "    # path to save logs\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    # save our best checkpoint by IoU metric\n",
    "    main_metric=\"dice\",\n",
    "    # IoU needs to be maximized.\n",
    "    minimize_metric=False,\n",
    "    # prints train logs\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from catalyst.dl import Callback, CallbackOrder, IRunner\n",
    "\n",
    "class CustomInferCallback(Callback):\n",
    "    def __init__(self, path=Path(r'logs\\test_output')):\n",
    "        super().__init__(CallbackOrder.Internal)\n",
    "        self.path = path\n",
    "        \n",
    "    def on_loader_start(self, runner: IRunner):\n",
    "        self.file_counter = 0\n",
    "        self.errors = []\n",
    "\n",
    "    def on_batch_end(self, runner: IRunner):\n",
    "        # data from the Dataloader\n",
    "        # image, mask = runner.input[\"image\"], runner.input[\"mask\"]\n",
    "        \n",
    "        #Get_coords(mask)\n",
    "        inputs = runner.input[\"image\"].detach().cpu()\n",
    "        images = utils.tensor_to_ndimage(inputs)\n",
    "        \n",
    "        output_masks = runner.output[\"logits\"].sigmoid().detach().cpu().numpy()\n",
    "        \n",
    "        print(runner.output.keys())\n",
    "        for img, mask in zip(images,output_masks):\n",
    "            points = Get_coords(mask[0])\n",
    "            img = cv2.UMat(img)\n",
    "            for point in points:\n",
    "                cv2.circle(img, (point[1],point[0]), 10, (0,1,0), -1)\n",
    "                \n",
    "            f_name = str(self.file_counter).zfill(4)\n",
    "            img = np.around(img.get()*255).astype(np.uint8)\n",
    "            cv2.imwrite(str(self.path/f_name)+'.png',img)\n",
    "            \n",
    "            self.file_counter += 1\n",
    "            if len(points) != 4:\n",
    "                self.errors.append(f_name)\n",
    "                \n",
    "                \n",
    "    def on_loader_end(self, runner: IRunner):\n",
    "        print('Prediction errors in this files:')\n",
    "        print(self.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint ./logs/segmentation/checkpoints/best.pth\n",
      "loaded state checkpoint ./logs/segmentation/checkpoints/best.pth (global epoch 10, epoch 10, stage train)\n",
      "dict_keys(['logits'])\n",
      "dict_keys(['logits'])\n",
      "dict_keys(['logits'])\n",
      "dict_keys(['logits'])\n",
      "dict_keys(['logits'])\n",
      "Prediction errors in this files:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SegmentationDataset(\n",
    "      images = np.load('idchess_task/xtest.npy'),\n",
    "      transforms = post_transform()\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "      test_dataset,\n",
    "      batch_size=1,\n",
    "      shuffle=None,\n",
    "      num_workers=0,\n",
    "    )\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from collections import OrderedDict\n",
    "from catalyst.dl import CheckpointCallback\n",
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "logdir = \"./logs/segmentation\"\n",
    "infer_loaders = {\"test\": test_loader}\n",
    "#infer_loaders = {\"test\": loaders['valid']}\n",
    "model = smp.FPN(encoder_name=\"efficientnet-b1\", classes=1)\n",
    "\n",
    "runner = SupervisedRunner(device=utils.get_device(), input_key=\"image\", input_target_key=\"mask\")\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=infer_loaders,\n",
    "    callbacks=OrderedDict([\n",
    "        (\"loader\", CheckpointCallback(resume=f\"{logdir}/checkpoints/best.pth\")),\n",
    "        (\"test\", CustomInferCallback())\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Идеи для улучшения:\n",
    "\n",
    "Ауги с поворотами:\n",
    "    Можно паддить до (256*256/2)**0.5*2 и вертеть на 360 градусов. \n",
    "    Можно написать аккуратные повороты (строим вектор от центра поворота до искомого угла доски, \n",
    "                                        и определяем для него максимальный угол поворота).\n",
    "    Геометрические сжатия\\растяжения.\n",
    "    \n",
    "Можно вообще попробовать искать прямые линии преобразованием Хафа и искать их пересечения,\n",
    "и (возможно) обойтись классическим CV.\n",
    "\n",
    "Можно натянуть маску на поверхность доски, так поучить и уже на предикт маске искать углы.\n",
    "\n",
    "Для оценки разных моделей можно написать функцию L2 метрики между предикт точками и действительными (для этого придется отсортировать предикт точки, как было в Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
